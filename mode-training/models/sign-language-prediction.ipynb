{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83bb603c-edd3-4c8f-853a-7f544243036f",
   "metadata": {},
   "source": [
    "# Sign Langauge Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0738081d-35f6-42d7-8f04-84133244ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import zipfile\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293c0881-0ed7-4e08-bb7b-bd958c8cbe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_classes = np.array([\n",
    "    'a', 'about', 'aim', 'all', 'and', 'audio', 'b', 'barrier', 'break', \n",
    "    'c', 'can', 'communication', 'creative', 'd', 'detect', 'developed', \n",
    "    'e', 'f', 'g', 'h', 'have', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'our', \n",
    "    'p', 'project', 'q', 'r', 's', 'sign language', 'solution', 't', 'team', \n",
    "    'text', 'that', 'to', 'translate', 'u', 'v', 'w', 'what', 'x', 'y', 'you', 'z'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce07a3b9-5c33-42ef-b344-8add628a1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands and Pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c02cb99-8131-4b28-aaec-2d8bf92f811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands and Pose\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=2,\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990530e5-080b-4ff5-ab06-ab2ecf05df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"models/my_vit_model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9320c44a-65bd-434f-aa20-0c1567dd0942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ layer_normalization_6         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">252</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ multi_head_attention_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">259,710</span> │ layer_normalization_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)          │                           │                 │ layer_normalization_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│                               │                           │                 │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ layer_normalization_7         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">252</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,024</span> │ layer_normalization_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_average_pooling1d_3    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ global_average_pooling1d_… │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,450</span> │ dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m126\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ layer_normalization_6         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m126\u001b[0m)           │             \u001b[38;5;34m252\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ multi_head_attention_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m126\u001b[0m)           │         \u001b[38;5;34m259,710\u001b[0m │ layer_normalization_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)          │                           │                 │ layer_normalization_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m126\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ multi_head_attention_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m126\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│                               │                           │                 │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ layer_normalization_7         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m126\u001b[0m)           │             \u001b[38;5;34m252\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │          \u001b[38;5;34m65,024\u001b[0m │ layer_normalization_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m131,328\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_average_pooling1d_3    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │          \u001b[38;5;34m32,896\u001b[0m │ global_average_pooling1d_… │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                │           \u001b[38;5;34m6,450\u001b[0m │ dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">495,914</span> (1.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m495,914\u001b[0m (1.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">495,912</span> (1.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m495,912\u001b[0m (1.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = tf.keras.models.load_model(model_save_path)\n",
    "\n",
    "# Verify the model structure\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9ae84e-9a87-4490-a4cd-37140b332c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update constants\n",
    "NUM_HANDS = 2\n",
    "NUM_LANDMARKS = 21\n",
    "FEATURES = 3  # x, y, z\n",
    "TIMESTEPS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16d9992-de2c-4589-be2f-e782a4543f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero placeholder\n",
    "ZERO_LANDMARK = [[0, 0, 0]] * NUM_LANDMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b8f8180-814e-4509-8411-4cbabd636c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(frame_rgb):\n",
    "    \"\"\"\n",
    "    Process the frame to extract hand and pose landmarks.\n",
    "    :param frame_rgb: RGB frame from video\n",
    "    :return: Hand landmarks, Pose landmarks\n",
    "    \"\"\"\n",
    "    hand_results = hands.process(frame_rgb)\n",
    "    return hand_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73027060-2989-43b3-a77a-1f48ef4a7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(frame, hand_results):\n",
    "    \"\"\"\n",
    "    Draw hand and pose landmarks on the frame.\n",
    "    :param frame: Original frame from video\n",
    "    :param hand_results: Hand landmarks from MediaPipe\n",
    "    :return: Frame with drawn landmarks\n",
    "    \"\"\"\n",
    "    # Draw hand landmarks\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2)\n",
    "            )\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "249815f9-ebde-4a15-99ca-bee6b2f19920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved scaler\n",
    "scaler = joblib.load(scaler_save_path)\n",
    "print(\"Scaler loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a65d1bc4-717e-47be-9d81-d592892a7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_landmarks(captured_landmarks):\n",
    "    \"\"\"\n",
    "    Normalize landmarks and maintain 3D structure for model input.\n",
    "    \"\"\"\n",
    "    # Convert to a numpy array\n",
    "    captured_landmarks = np.array(captured_landmarks)  # Shape: (50, 2, 21, 3)\n",
    "    \n",
    "    # Reshape to (timesteps, features) where features = NUM_HANDS * NUM_LANDMARKS * 3\n",
    "    reshaped_landmarks = captured_landmarks.reshape(TIMESTEPS, NUM_HANDS * NUM_LANDMARKS * 3)\n",
    "    \n",
    "    # Normalize between 0 and 1 (optional)\n",
    "    normalized_landmarks = reshaped_landmarks / np.max(reshaped_landmarks)\n",
    "    \n",
    "    return normalized_landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dadb4d7c-3d20-4dca-8db5-0174b4c30282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data capture variables\n",
    "captured_landmarks = []  # List to store landmarks for all frames\n",
    "frame_counter = 0  # Tracks the number of frames collected\n",
    "recording = False  # Indicates when to start recording frames\n",
    "hands_detected_once = False  # Flag to track if hands were detected at least once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e65f1a-d961-4c2f-9226-388a3b30181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hands detected. Starting to collect frames.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "Predicted class index: 28, Number of classes: 50\n",
      "our\n",
      "Hands detected. Starting to collect frames.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Predicted class index: 23, Number of classes: 50\n",
      "k\n",
      "Hands detected. Starting to collect frames.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Predicted class index: 28, Number of classes: 50\n",
      "our\n"
     ]
    }
   ],
   "source": [
    "# Video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame for a mirrored view\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get landmarks\n",
    "    hand_results = get_landmarks(frame_rgb)\n",
    "\n",
    "    # Draw landmarks on the frame\n",
    "    frame = draw_landmarks(frame, hand_results)\n",
    "\n",
    "    # Check if hands are detected\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        if not recording and not hands_detected_once:\n",
    "            # Start recording if hands are detected\n",
    "            print(\"Hands detected. Starting to collect frames.\")\n",
    "            hands_detected_once = True\n",
    "            recording = True\n",
    "\n",
    "        # Extract landmarks for detected hands\n",
    "        frame_landmarks = [ZERO_LANDMARK] * NUM_HANDS\n",
    "        for idx, hand_landmark in enumerate(hand_results.multi_hand_landmarks):\n",
    "            if idx < NUM_HANDS:\n",
    "                frame_landmarks[idx] = [[lm.x, lm.y, lm.z] for lm in hand_landmark.landmark]\n",
    "\n",
    "        # Append frame landmarks\n",
    "        captured_landmarks.append(frame_landmarks)\n",
    "        frame_counter += 1\n",
    "    elif recording:\n",
    "        # If no hands are detected, use zero matrix\n",
    "        captured_landmarks.append([ZERO_LANDMARK] * NUM_HANDS)\n",
    "        frame_counter += 1\n",
    "\n",
    "    if recording and frame_counter >= TIMESTEPS:\n",
    "        # Preprocess collected landmarks\n",
    "        preprocessed_input = preprocess_landmarks(captured_landmarks)\n",
    "\n",
    "        # Add batch dimension for prediction\n",
    "        preprocessed_input = np.expand_dims(preprocessed_input, axis=0)\n",
    "\n",
    "        # Predict\n",
    "        prediction = model.predict(preprocessed_input)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        confidence = np.max(prediction)\n",
    "\n",
    "        # Ensure the predicted_class is within the valid range\n",
    "        print(f\"Predicted class index: {predicted_class}, Number of classes: {len(word_classes)}\")\n",
    "\n",
    "        # Access the predicted word\n",
    "        if predicted_class < len(word_classes):\n",
    "            predicted_word = word_classes[predicted_class]\n",
    "            print(predicted_word)\n",
    "        else:\n",
    "            print(\"Error: predicted_class out of bounds.\")\n",
    "\n",
    "        # Display the prediction\n",
    "        cv2.putText(frame, f\"Predicted: {predicted_class}, Confidence: {confidence:.2f}\",\n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Reset data for the next cycle\n",
    "        captured_landmarks = []\n",
    "        frame_counter = 0\n",
    "        recording = False\n",
    "        hands_detected_once = False\n",
    "        time.sleep(3)  # Brief pause before next collection cycle\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Hand Gesture Prediction', frame)\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
